\levelstay{Vector space basics}

In this section we define and study the basic properties of vectors.
It is assumed that the reader has learned about vectors before, so we focus on examples of vector spaces that may be new.

\leveldown{Definition}

\newtheorem{definition}{Definition}
\begin{definition}[Vector Space]
A vector space \textbf{V} consists of three things: a collection of vectors $\{\ket{v}\}$, an addition operation denoted by $+$, and a scalar field $\Scalars$ with the following properties: \footnote{A scalar field is a set of objects that can be added and multiplied such that every element has both an additive and multiplicative inverse.
The most common examples are the real numbers $\reals$ and the complex numbers $\complexes$.}
\begin{enumerate}
  \item If $\ket{v}$ and $\ket{w}$ are both vectors in \textbf{V} then their sum $\ket{v} + \ket{w}$ is another vector in \textbf{V}.
In symbols we could write,
  \begin{equation*}
    \forall \, \ket{v}, \ket{w} \in \textbf{V}, \ket{v} + \ket{w} \in \textbf{V} \, .
  \end{equation*}
  In English this equation is read ``The sum of the vector named `v' and another vector named `w' is also a vector''.

  \item Multiplication of a vector by a scalar yields a vector: if $\ket{v}$ is a vector in \textbf{V} and $a$ is a scalar in \textbf{S}, then $a\ket{v}$ is a vector in \textbf{V}.

  \item There is a vector $0$ called ``zero'' or ``the zero vector'' such that for any vector $\ket{v}$ in \textbf{V} we have $\ket{v} + 0 = \ket{v}$.\footnote{Note that we don't put the $\ket{\,}$ symbol around $0$.
  This is because $\ket{0}$ is often used to denote a vector which is not the zero vector.
  In particular, $\ket{0}$ often refers to the ground state of a quantum system.
  The symbol $0$ also refers to the zero element in the field $\Scalars$, this is ok because multiplying any vector by the zero element of $\Scalars$ yields the zero vector.}

  \item For every vector $\ket{v}$ there is an additive inverse of $\ket{v}$ denoted $\ket{-v}$ that satisfies the equation $\ket{v} + \ket{-v} = 0$.

  \item All the usual associativity and distributativity rules hold.
  For example,
  \begin{equation*}
    (a+b)(\ket{v} + \ket{w}) = a\ket{v} + a\ket{w} + b\ket{v} + b\ket{w}
  \end{equation*}
\end{enumerate}
\end{definition}

%The usual way to refer to a vector space $\VS$ consisting of vectors $\{\ket{v}\}$ with scalars $\Scalars$ is to say that ``$\VS$ is the vector space of $\ket{v}$ over $\Scalars$.''
%A vector space over the real numbers is called a \textbf{real vector space}, while a vector space over the complex numbers is called a \textbf{complex vector space}.

The symbol $\ket{\phantom{v}}$, called a \textbf{ket}, indicates a vector.
The symbol inside the ket, whether it be the letter $v$, the name ``Victor'', or any other symbol, is just a name for the vector.
By convention, the vector $\ket{v} + \ket{w}$ can be denoted $\ket{v+w}$.
The name of the vector is often chosen to convey useful information about the vector itself, but keep in mind that the thing in the ket is ultimately just a label.

Two other notations for vectors in common use are $\vec{v}$ and $\mathbf{v}$.
We use the ket notation because it's unambiguous common in physics literature, and lends itself to a useful expression for inner products as we will see below.

\levelstay{Examples of vector spaces}

\textbf{1. The set of all real valued functions defined on the interval $[0, 1]$ over the field of the real numbers,} i.e. the functions $[0, 1] \to \reals$.
Let's explicitly check that this set satisfies the definition of a vector space.
\begin{enumerate}
  \item[\textit{1}.] Given two functions $f: [0, 1] \to \reals$ and $g: [0, 1] \to \reals$, their sum $h = f + g$ defined by the equation $h(x) = f(x) + g(x)$ is also a function $[0, 1] \to \reals$.
  \item[\textit{2}.] Multiplying a function $f: [0, 1] \to \reals$ by a scalar $c$ produces a new function $g: [0, 1] \to \reals$ defined by the equation $g(x) = c f(x)$.
  \item[\textit{3}.] The zero vector is the function $z:[0, 1] \to \reals$ defined by $z(x) = 0$. This is seen because if we define the sum $h = f + z$, then $h(x) = f(x) + z(x) = f(x)$, so $h = f$.
  \item[\textit{4}.] For any function $f:[0, 1] \to \reals$, its inverse is the function $g:[0, 1] \to \reals$ defined by the equation $g(x) = -f(x)$. This is seen because $(f + g)(x) = f(x) + g(x) = 0$, so $f + g = z$.
  \item[\textit{5}.] Associativity when summing functions and distributativity of multiplying them by scalars works as expected: $f + (g + h) = (f + g) + h$, and $a(f + g) = af + ag$.
\end{enumerate}

\noindent \textbf{2. The set of complex numbers $\ket{z}$, over the field of real numbers.}
In the previous example, we wrote functions without the vector symbol to keep notation familiar, but here we introduce it.
Checking the vector space properties is easy.
Given two complex numbers $\ket{z}$ and $\ket{u}$, their sum $\ket{z} + \ket{u}$ is also a complex number.
Given a complex number $\ket{z}$ and a real number $r$, the product $r\ket{z}$ is a complex number.
The rest of the vector space properties also work, as you should check.\newline

\noindent \textbf{3. The set of all polynomials of degree seven, over the real numbers.}
We check the summation property:
\begin{equation*}
  \begin{split}
    p =& p_7\,x^7 + \cdots + p_0 \\
   +q =& q_7\,x^7 + \cdots + q_0 \\
    \hline
      =& (p_7 + q_7) \, x^7 + \cdots + (p_0 + q_0)
  \end{split}
\end{equation*}
The rest of the properties are straightforward to check.\newline

\noindent \textbf{4. The set of ordered n-tuples of real numbers ($x_1$, $x_2$,\ldots , $x_n$), over the real numbers.}
In this case we define addition by the equation
\begin{equation*}
  (x_1, x_2, \ldots, x_n) + (y_1, y_2, \ldots, y_n) = (x_1 + y_1, x_2 + y_2, \ldots, x_n + y_n) \, .
\end{equation*}
This space is called $\reals^n$.

\levelstay{Isomorphism to $\reals^n$}
The vector space in example 4 is particularly interesting and important because it can be used to represent any finite dimensional real vector space.
For example, the vector space of example 2, the complex numbers over the reals, can be represented by the set of 2-tuples where the first entry represents the real part and the second entry represents the imaginary part.
In this representation a complex number $z$ is represented as a tuple
\begin{equation*}
  z = x + iy \sim (x,y)
\end{equation*}
where the symbol $\sim$ means ``is represented by''.
Obviously the way to add two vectors in this representation is by adding the components, because two complex numbers are added by adding their real and imaginary parts.

The same is true for example 3, the set of all polynomials of degree seven over the real numbers.
Given two such polynomials
\begin{equation*}
  p = p_7 \, x^7 + p_6 \, x^6 + \cdots + p_0 \qquad
  q = q_7 \, x^7 + q_6 \, x^6 + \cdots + q_0
\end{equation*}
their sum is
\begin{equation*}
  p + q = (p_7 + q_7) x^7 + (p_6 + q_6)x^6 + \cdots + (p_0 + q_0)
  \, .
\end{equation*}
We add the coefficients of each degree of the polynomials, just as if we were adding the components in tuples of real numbers.
Therefore, we can represent the vector space of ``polynomials of degree seven over the real numbers'' by the vector space of ``8-tuples of real numbers, over the real numbers''.
In that representation, we would have written the sum like this
\begin{equation*}
  \ket{p + q}
  \sim (p_7, p_6, \ldots, p_0) + (q_7, q_6, \ldots, q_0) = (p_7 + q_7, p_6 + q_6, \ldots, p_0 + q_0)
  \, .
\end{equation*}

In formal mathematical language, any finite dimensional vector space over the real numbers is \textbf{isomorphic} to $\reals^n$.
Two things being isomorphic means that you can map between those two things, and that this mapping preserves structure.
For example, if we represent two polynomials of degree seven $p$ and $q$ by two 7-tuples of real numbers, then the sum of those tuples should be the representation of $p + q$.
Ironically, it's obvious that this condition holds because we use that isomorphism all the time when we compute sums of polynomials.
That is, we sum the coefficients of the polynomial, order by order, just as we would sum the entries in a tuple slot by slot.

This isomorphism between any finite dimensional vector space over the real numbers and the specific vector space $\reals^n$ is profoundly important for two reasons.
First, the correspondence gives us a trivial way to compute the sum of two vectors, i.e. by summing the elements in the $n$-tuples representing each vector.
Second, and more importantly, remember the example of the arrow on a page in the introduction.
There we saw that the arrow may be represented by more than one element of $\reals^2$.
Now that we know that any finite dimentional vector space over the reals can be represented by $\reals^n$, we may ask whether a vector can be represented by different elements in $\reals^n$ depending on the choice of coordinates, just like the arrow.
Indeed, this thought is what will lead us to solve the coupled oscillators problem in this chapter, and to discover the Fourier transform in the next chapter.

\begin{flushleft} $\clubsuit$ \end{flushleft}
The tuple space $\reals^n$ can be used to represent configurations or ``states'' of physical systems.
The configuration of our box and spring system is given by two numbers: the displacements $x_1$ and $x_2$ of the masses from their equilibrium positions.
Knowledge of these two numbers for all times is the solution of the problem we want to find.
In other words, we say that we've solved the problem if we find the functions
\begin{equation*}
  x_1(t) \qquad \textrm{and} \qquad x_2(t) \, .
\end{equation*}
We see that the state of the system at time $t$ is a vector space if we collect $x_1(t)$ and $x_2(t)$ into a 2-tuple $(x_1(t), x_2(t))$.
Since we showed that tuples of numbers represent vector spaces, we now see that the set of configurations of our spring-and-box system is actually a vector space, and that observation will soon give us a beautiful way to solve for the system's motion.
\begin{flushright} $\clubsuit$ \end{flushright}

\levelstay{Bases and dimension}

Vector space dimension was mentioned in the previous section.
Here we define basis and dimension.

\begin{definition}[Basis]
Consider a finite dimensional vector space \VS.
A basis for \VS, denoted by the symbol $e$, is a set of vectors
\begin{displaymath}
  e \equiv \{\ket{e_1}, \ket{e_2}, \ldots, \ket{e_n}\}
\end{displaymath}
with the following three properties:
\begin{enumerate}
  \item
    For each $i$, $\ket{e_i}$ is a member of \VS.
  \item
    Any vector $\ket{v}$ in \textbf{V} can be represented as a linear combination,
    \begin{displaymath}
      \ket{v}
      = v^e_1 \ket{e_1} + v^e_2 \ket{e_2} + \cdots + v^e_n \ket{e_n} = \sum_{i=1}^n v^e_i \ket{e_i}
    \end{displaymath}
    where the numbers $v_i^e$ are scalars.
    The set of scalars $v_i^e$ are called the \textbf{coefficients} or \textbf{components} of $\ket{v}$ in the $e$ basis.
    The linear combination is called the ``decomposition of $\ket{v}$ in basis $e$''.
  \item
    The linear combination in 2 is unique.
\end{enumerate}
\end{definition}
In the definition above, the letter $e$ refers to the \textit{set} of vectors $\{\ket{e_i}\} \equiv \{ \ket{e_1} \ldots \ket{e_n} \}$ while the symbol $\ket{e_i}$ refers to \emph{one} particular basis vector.
Some books use $\ket{e_i}$ (or perhaps $\mathbf{e}_i$ or $\vec{e}_i$) to refer to the basis itself.
This is abuse of notation because it confuses a single element with a whole set.
In this document, we use either a letter with no subscript, such as $e$, or the set notation $\{\ket{e_i}\}$, to refer to a basis.
Of course, not all bases will be named $e$, we could name a basis $f$, $G$, \textit{Victor}, or anything we want.

\begin{definition}[Dimension]
  Given a vector space \VS\, with basis $e$, the dimension of \VS\, is the number of elements in the set $e$.
\end{definition}
This definition of dimension might seem to depend on the choice of basis, but you can prove that for a given vector space every basis has the same number of elements.

\leveldown{Examples of bases and dimension}

\textbf{1.} The vectors $\ket{e_1} = \ket{1} = 1$ and $\ket{e_2} = \ket{i} = i$ form a basis for the vector space of complex numbers over the real numbers.
\emph{1}. $1$ and $i$ are both complex numbers, \emph{2}. Any complex number $z$ can be decomposed as
\begin{displaymath}
  z = \Re(z) \ket{1} + \Im(z) \ket{i}
\end{displaymath}
where $\Im(z)$ and $\Re(z)$ are both real numbers, \emph{3}. The decomposition is obviously unique.
As there are two basis vectors, the space of complex numbers over the real numbers has dimension two.\footnote{In this example the vector space has dimension two because we're using the reals as the field.
If we take the complex numbers over the complex numbers, then a basis could be formed by just $\ket{1}$, because any complex number $z$ could be decomposed as $z = z\,\ket{1}$, and in this case, the space has dimension one.}\newline

\noindent \textbf{2.} The polynomials $\{1, x,x^{2}\}$ form a basis for the space of polynomials of degree two: by definition any polynomial of degree two has the form $a_0 + a_1 x + a_2 x^2$ where the coefficients $a_i$ are uniquely determined by the polynomial in question.
The dimension is 3.\newline

\noindent \textbf{3.} For the vector space $\mathbb{R}^n$ we have the obvious basis
\begin{displaymath}
  (1,0,\ldots ,0), (0,1,\ldots ,0), \ldots , (0,0,\ldots ,1)
\end{displaymath}
The dimension of $\mathbb{R}^n$ is $n$.\newline

\noindent Now let's apply this idea of bases to our physical problem.
\begin{flushleft} $\clubsuit$ \end{flushleft}
We already pointed out that the numbers $x_1(t)$ and $x_2(t)$ can be put into a tuple and considered an element in $\mathbb{R}^2$
So, the configuration space of the box-and-spring system is a vector space, but how is this \emph{useful}?
Well, we can think of equation (\ref{eq:Newton}) as a matrix equation,
\begin{equation} \label{eq:equationOfMotionMatrix}
\left[ \begin{array}{c} \ddot{x}_1(t) \\ \ddot{x}_2(t) \end{array} \right] = \omega^{2}_{0}\left[ \begin{array}{cc} -2 & 1 \\ 1 & -2 \end{array} \right] \left[ \begin{array}{c} x_{1}(t) \\ x_{2}(t) \end{array} \right] \, .
\end{equation}
This matrix equation is exactly equivalent to (\ref{eq:Newton}) as you can check by carrying out the matrix multiplication.
In this matrix equation, The numbers $x_1(t)$ and $x_2(t)$ can be regarded as the components of $\ket{\Psi (t)}$ in a basis $X$ consisting of two vectors $\ket{X_1}$ and $\ket{X_2}$.
$\ket{X_1}$ corresponds to a configuration of the system in which the first mass is displaced one unit of distance to the right of its equilibrium position and mass number two is at it's equilibrium position, and $\ket{X_2}$ is defined analogously.
In this basis, the configuration is written
\begin{equation} \label{eq:Psirep2}
\ket{\Psi (t)} = x_{1}(t)\ket{X_1} + x_{2}(t)\ket{X_2} \, .
\end{equation}

The fact that the differential equations in (\ref{eq:Newton}) are coupled manifests itself in the off-diagonal elements in the matrix of (\ref{eq:equationOfMotionMatrix}).
This suggests that \emph{if we could find a different basis in which the matrix were diagonal, then the differential equations would decouple and the problem would be easily solved}.
Setting up the mathematical tools to understand how to find such a basis is the object of the next section.

There is one more important remark to be made before we move on.
It might not make any sense at this point, but read it anyway before going on to the next section.
If it doesn't make sense now don't worry, it will soon.
Equation (\ref{eq:equationOfMotionMatrix}) is a matrix equation written down in a particular basis $X$.
We could rewrite the equation in a more abstract form without reference to any basis.
Let $\ket{\ddot{\Psi}(t)}$ be a vector representing the acceleration of the system at time $t$.
Then we can write
\begin{equation}\label{eq:equationOfMotionAbstract}
\ket{\ddot{\Psi}(t)} = P \ket{\Psi (t)}
\end{equation}
where $P$ is a linear transformation whose \emph{representation} in basis $X$ is given by the matrix shown in equation (\ref{eq:equationOfMotionMatrix}).
It is crucial that you understand that $P$ is not the matrix in equation (\ref{eq:equationOfMotionMatrix}).
It is some linear transformation on the vector space of configurations of the physical system.
It is \emph{represented} by the matrix in equation (\ref{eq:equationOfMotionMatrix}) in the basis $X$.
In order to decouple the differential equations we want to find a different basis $Y$ in which the matrix representation of $P$ is diagonal.
\begin{flushright} $\clubsuit$ \end{flushright}

\levelstay{Exercises}
\begin{itemize}
\item[1)] Show that if we work in a new basis $Y$ in which the matrix in equation (\ref{eq:equationOfMotionMatrix}) were diagonal then the equations decouple and are easily solved.
\end{itemize}

