\levelstay{Vector space basics}
In this section we define and study the basic objects with which we will work.
It is assumed that the reader has already learned about these basic objects and so we focus on more complicated examples that may be new.

\leveldown{Definition}

\newtheorem{definition}{Definition}
\begin{definition}[Vector Space]
A vector space \textbf{V} consists of three things: a collection of vectors $\{\ket{v}\}$, an addition operation denoted by $+$, and a scalar field \textit{\textbf{S}}.\footnote{A scalar field is a set of objects that can be added and multiplied such that every element has both an additive and multiplicative inverse.
The most common examples are the real numbers $\mathbb{R}$ and the complex numbers $\mathbb{C}$.}
The following properties must be satisfied:
\begin{enumerate}
\item If $\ket{v}$ and $\ket{w}$ are both vectors in \textbf{V} then their sum $\ket{v} + \ket{w}$ is another vector in \textbf{V}.
In symbols we could write,
\begin{equation*}
\forall \, \ket{v}, \ket{w} \in \textbf{V}, \ket{v+w} \equiv \ket{v} + \ket{w} \in \textbf{V} \, .
\end{equation*}
In English this equation is read ``The sum of the vector named `v' and another vector named `w' is a third vector named `$v+w$'.''

\item Multiplication of a vector by a scalar yields a vector: if $\ket{v}$ is a vector in \textbf{V} and $a$ is a scalar in \textbf{S}, then $a\ket{v}$ is a vector in \textbf{V}.

\item There is a vector $0$ called ``zero'' or ``the zero vector'' such that for any vector $\ket{v}$ in \textbf{V} we have $\ket{v} + 0 = \ket{v}$.\footnote{Note that we don't put the $\ket{\,}$ symbol around $0$.
This is because $\ket{0}$ is often used to denote a vector which is \emph{not the zero vector!}
The symbol $0$ also refers to the zero element in the field $\textit{\textbf{S}}$, this is ok because multiplying any vector by the zero element of $\textit{\textbf{S}}$ yields the zero vector.}

\item For every vector $\ket{v}$ there is an additive inverse of $\ket{v}$ denoted $\ket{-v}$ that satisfies the equation $\ket{v} + \ket{-v} = 0$.

\item All the usual associativity and distributativity rules hold.
For example,
\begin{equation*}
(a+b)(\ket{v} + \ket{w}) = a\ket{v} + a\ket{w} + b\ket{v} + b\ket{w} \qquad \heartsuit
\end{equation*}
\end{enumerate}
\end{definition}

The usual way to refer to a vector space $\VS$ consisting of vectors $\{\ket{v}\}$ with scalars \textit{\textbf{S}} is to say that $\VS$ is the \textbf{vector space of} $\ket{v}$ \textbf{over} \textit{\textbf{S}}.
A vector space over the real numbers is called a \textbf{real vector space}, while a vector space over the complex numbers is called a \textbf{complex vector space}.

The symbol $\ket{\phantom{v}}$, called a \textbf{ket}, indicates a vector.
The symbol inside the ket, whether it be the letter $v$, the name ``John'', or any other symbol, is just a name for the vector.
Keep in mind that $\ket{v+w}$ is simply a shorthand for writing the vector that is the sum $\ket{v} + \ket{w}$.
The symbol ``$v+w$'' inside the ket is just a label.
Two other notations for vectors in common use are $\vec{v}$ and $\mathbf{v}$.
We use the ket notation because it's unambiguous and very common in physics.

\levelstay{Examples of vector spaces}
\begin{enumerate}
\item The set of all real valued functions defined on the interval $[0, 1]$ over the field of the real numbers.
This is a vector space because \emph{1}. If $f$ and $g$ are functions defined on $[0, 1]$ then the function $h = f + g$ is also a function defined on $[0, 1]$, \emph{2}. Multiplication by scalars also produces functions on $[0, 1]$, \emph{3}. The zero vector is the function defined by $f(x)=0$, \emph{4}. The inverse of $f$ is the function $g$ defined by the equation $g(x) = -f(x)$, and \emph{5}. Associativity and distributativity work as expected.
\item The set of complex numbers $\ket{z}$, over the field of real numbers.
Given two complex numbers $\ket{z}$ and $\ket{u}$, then $\ket{z+u} \equiv \ket{z} + \ket{u}$ is also a complex number.
The rest of the vector space properties also work, as you should check.
\item The set of all polynomials of degree seven, over the real numbers.
\item The set of ordered n-tuples of real numbers ($x_1$, $x_2$,\ldots , $x_n$), over the real numbers.
In this case we define addition by the equation
\begin{equation*}
(x_1, x_2, \ldots, x_n) + (y_1, y_2, \ldots, y_n) = (x_1 + y_1, x_2 + y_2, \ldots, x_n + y_n) \, .
\end{equation*}
This space is called $\mathbb{R}^n$.
\end{enumerate}
The vector space in example 4 is particularly interesting and important because it can be used to represent \emph{any} finite dimensional real vector space imaginable (we'll talk about this in depth in the next section).
For example, the vector space of example 2, the complex numbers over the reals, can be represented by the set of 2-tuples where the first entry represents the real part and the second entry represents the imaginary part.
In this representation a complex number $z$ is written
\begin{equation*}
z = x + iy \sim (x,y)
\end{equation*}
where the symbol $\sim$ means ``is represented by''.
Obviously the way to add two vectors in this representation is by adding the components, because two complex numbers are added by adding their real and imaginary parts.

\begin{flushleft} $\clubsuit$ \end{flushleft}
Importantly, the tuple space $\mathbb{R}^n$ can be used to represent configurations or \textit{states} of physical systems.
The configuration of our box and spring system is given by two numbers: the displacements $x_1$ and $x_2$ of the masses from their equilibrium positions.
Knowledge of these two numbers for all times is the solution of the problem we want to find.
In other words, we say that we've solved the problem if we find the functions
\begin{equation*}
x_{1}(t) \qquad \textrm{and} \qquad x_{2}(t) \, .
\end{equation*}
We denote the configuration of the system at time $t$ by the symbol
\begin{displaymath}
\ket{\Psi (t)} \, .
\end{displaymath}
Since our system is characterised by the positions of two masses the configuration $\ket{\Psi (t)}$ can be represented by a 2-tuple,
\begin{equation} \label{eq:Psirep}
\ket{\Psi (t)} \sim (x_{1}(t),x_2(t))
\end{equation}
where again $\sim$ means ``is represented by''.
This is no big revelation; we already knew that the spring and box system could be represented by $x_t(t)$ and $x_2(t)$.
However, since we showed that tuples of numbers represent vector spaces, we now see that the configuration of our spring-and-box system is actually a vector, and that observation will soon give us a beautiful way to solve for the system's motion.
\begin{flushright} $\clubsuit$ \end{flushright}

\levelstay{Bases and Dimension}

Here we define the notions of vector spaces basis and dimension.

\begin{definition}[Basis]
Consider a finite dimensional vector space \VS.
A basis for \VS, denoted by the symbol $e$, is a set of vectors
\begin{displaymath}
e \equiv \{\ket{e_1}, \ket{e_2}, \ldots, \ket{e_n}\}
\end{displaymath}
with the following three properties:
\begin{enumerate}
\item
For each $i$, $\ket{e_i}$ is a member of \VS.
\item
Any vector $\ket{v}$ in \textbf{V} can be represented as a linear combination,
\begin{displaymath}
\ket{v} = v^e_1 \ket{e_1} + v^e_2 \ket{e_2} + \cdots + v^e_n \ket{e_n} = \sum_{i=1}^n v^e_i \ket{e_i}
\end{displaymath}
where the numbers $v_i^e$ are scalars.
The set of scalars $v_i^e$ are called the \textbf{coefficients} or \textbf{components} of $\ket{v}$ in the $e$ basis.
The linear combination is called the ``decomposition of $\ket{v}$ in basis $e$''.
\item
The linear combination in 2 is unique. \qquad $\heartsuit$
\end{enumerate}
\end{definition}
Note that in the definition above, the letter $e$ refers to the \textit{set} of vectors $\{\ket{e_i}\} \equiv \{ \ket{e_1} \ldots \ket{e_n} \}$ while the symbol $\ket{e_i}$ refers to \emph{one} particular basis vector.
Some books use $\ket{e_i}$ (or perhaps $\mathbf{e}_i$ or $\vec{e}_i$) to refer to the basis itself.
This is abuse of notation because it confuses a single element with a whole set.
In this document, we use either a letter with no subscript, such as $e$, or the set notation $\{\ket{e_i}\}$, to refer to a basis.
Of course, not all bases will be named $e$, we could name a basis $f$, $G$, \textit{Victor}, or anything we want.

\begin{definition}[Dimension]
Given a vector space \VS with basis $e$ the dimension of \VS is the number of elements in the set $e$. \qquad $\heartsuit$
\end{definition}
This definition of dimension might seem to depend on the choice of basis, but you can prove that for a given vector space every basis has the same number of elements.

\leveldown{Examples of bases and dimension}

\begin{enumerate}
\item The vectors $\ket{e_1} = \ket{1} = 1$ and $\ket{e_2} = \ket{i} = i$ form a basis for the vector space of complex numbers over the real numbers.
\emph{1}. $1$ and $i$ are both complex numbers, \emph{2}. Any complex number $z$ can be decomposed as
\begin{displaymath}
z = \Re(z) \ket{1} + \Im(z) \ket{i}
\end{displaymath}
where $\Im(z)$ and $\Re(z)$ are both real numbers, \emph{3}. The decomposition is obviously unique.
As there are two basis vectors, the space of complex numbers over the real numbers is two dimensional.\footnote{In this example the vector space is two dimensional because we're using the reals as the field.
If we take the complex numbers over the complex numbers, then a basis could be formed by just $\ket{1}$, because any complex number $z$ could be decomposed as $z = z\,\ket{1}$.
In this case, the space is one dimensional.}
\item The polynomials $\{1, x,x^{2}\}$ form a basis for the space of polynomials of degree two: by definition any polynomial of degree two has the form $a_0 + a_1 x + a_2 x^2$ where the coefficients $a_i$ are uniquely determined by the polynomial in question.
The dimension is 3.
\item For the vector space $\mathbb{R}^n$ we have the obvious basis
\begin{displaymath}
(1,0,\ldots ,0), (0,1,\ldots ,0), \ldots , (0,0,\ldots ,1)
\end{displaymath}
The dimension of $\mathbb{R}^n$ is $n$.
\end{enumerate}

Now let's apply this idea of bases to our physical problem.
\begin{flushleft} $\clubsuit$ \end{flushleft}
We already pointed out that the numbers $x_1(t)$ and $x_2(t)$ can be put into a tuple and considered an element in $\mathbb{R}^2$
So, the configuration space of the box-and-spring system is a vector space, but how is this \emph{useful}?
We can think of equation (\ref{eq:Newton}) as a matrix equation,
\begin{equation} \label{eq:equationOfMotionMatrix}
\left[ \begin{array}{c} \ddot{x}_1(t) \\ \ddot{x}_2(t) \end{array} \right] = \omega^{2}_{0}\left[ \begin{array}{cc} -2 & 1 \\ 1 & -2 \end{array} \right] \left[ \begin{array}{c} x_{1}(t) \\ x_{2}(t) \end{array} \right] \, .
\end{equation}
This matrix equation is exactly equivalent to (\ref{eq:Newton}) as you can check by carrying out the matrix multiplication.
In this matrix equation, The numbers $x_1(t)$ and $x_2(t)$ can be regarded as the components of $\ket{\Psi (t)}$ in a basis $X$ consisting of two vectors $\ket{X_1}$ and $\ket{X_2}$.
$\ket{X_1}$ corresponds to a configuration of the system in which the first mass is displaced one unit of distance to the right of its equilibrium position and mass number two is at it's equilibrium position, and $\ket{X_2}$ is defined analogously.
In this basis, the configuration is written
\begin{equation} \label{eq:Psirep2}
\ket{\Psi (t)} = x_{1}(t)\ket{X_1} + x_{2}(t)\ket{X_2} \, .
\end{equation}

The fact that the differential equations in (\ref{eq:Newton}) are coupled manifests itself in the off-diagonal elements in the matrix of (\ref{eq:equationOfMotionMatrix}).
This suggests that \emph{if we could find a different basis in which the matrix were diagonal, then the differential equations would decouple and the problem would be easily solved}.
Setting up the mathematical tools to understand how to find such a basis is the object of the next section.

There is one more very important remark to be made before we move on.
It might not make any sense at this point, but read it anyway before going on to the next section.
If it doesn't make sense now don't worry, it will very soon.
Equation (\ref{eq:equationOfMotionMatrix}) is a matrix equation written down in a particular basis $X$.
We could rewrite the equation in a more abstract form without reference to any basis.
Let $\ket{\ddot{\Psi}(t)}$ be a vector representing the acceleration of the system at time $t$.
Then we can write
\begin{equation}\label{eq:equationOfMotionAbstract}
\ket{\ddot{\Psi}(t)} = P \ket{\Psi (t)}
\end{equation}
where $P$ is a linear transformation whose representation in basis $X$ is given by the matrix shown in equation (\ref{eq:equationOfMotionMatrix}).
It is crucial that you understand that $P$ is not the matrix in equation (\ref{eq:equationOfMotionMatrix}).
It is some linear transformation on the vector space of configurations of the physical system.
It is \emph{represented} by the matrix in equation (\ref{eq:equationOfMotionMatrix}) in the basis $X$.
In order to decouple the differential equations we want to find a different basis $Y$ in which the matrix representation of $P$ is diagonal.
\begin{flushright} $\clubsuit$ \end{flushright}

\levelstay{Exercises}
\begin{itemize}
\item[1)] Show that if we work in a new basis $Y$ in which the matrix in equation (\ref{eq:equationOfMotionMatrix}) were diagonal then the equations decouple and are easily solved.
\end{itemize}

