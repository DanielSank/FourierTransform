\levelstay{Vector space basics}

In this section we define and study the basic properties of vectors.
It is assumed that the reader has learned about vectors before, so we focus on examples of vector spaces that may be new.

\leveldown{Definition}

\newtheorem{definition}{Definition}
\begin{definition}[Vector Space]
A vector space \textbf{V} consists of three things: a collection of vectors $\{\ket{v}\}$, an addition operation denoted by $+$, and a scalar field $\Scalars$ with the following properties: \footnote{A scalar field is a set of objects that can be added and multiplied such that every element has both an additive and multiplicative inverse.
The most common examples are the real numbers $\reals$ and the complex numbers $\complexes$.}
\begin{enumerate}
  \item If $\ket{v}$ and $\ket{w}$ are both vectors in \textbf{V} then their sum $\ket{v} + \ket{w}$ is another vector in \textbf{V}.
In symbols we could write,
  \begin{equation*}
    \forall \, \ket{v}, \ket{w} \in \textbf{V}, \ket{v} + \ket{w} \in \textbf{V} \, .
  \end{equation*}
  In English this equation is read ``The sum of the vector named `v' and another vector named `w' is also a vector''.

  \item Multiplication of a vector by a scalar yields a vector: if $\ket{v}$ is a vector in \textbf{V} and $a$ is a scalar in \textbf{S}, then $a\ket{v}$ is a vector in \textbf{V}.

  \item There is a vector $0$ called ``zero'' or ``the zero vector'' such that for any vector $\ket{v}$ in \textbf{V} we have $\ket{v} + 0 = \ket{v}$.\footnote{Note that we don't put the $\ket{\,}$ symbol around $0$.
  This is because $\ket{0}$ is often used to denote a vector which is not the zero vector.
  In particular, $\ket{0}$ often refers to the ground state of a quantum system.
  The symbol $0$ also refers to the zero element in the field $\Scalars$, this is ok because multiplying any vector by the zero element of $\Scalars$ yields the zero vector.}

  \item For every vector $\ket{v}$ there is an additive inverse of $\ket{v}$ denoted $\ket{-v}$ that satisfies the equation $\ket{v} + \ket{-v} = 0$.

  \item All the usual associativity and distributativity rules hold.
  For example,
  \begin{equation*}
    (a+b)(\ket{v} + \ket{w}) = a\ket{v} + a\ket{w} + b\ket{v} + b\ket{w}
  \end{equation*}
\end{enumerate}
\end{definition}

%The usual way to refer to a vector space $\VS$ consisting of vectors $\{\ket{v}\}$ with scalars $\Scalars$ is to say that ``$\VS$ is the vector space of $\ket{v}$ over $\Scalars$.''
%A vector space over the real numbers is called a \textbf{real vector space}, while a vector space over the complex numbers is called a \textbf{complex vector space}.

The symbol $\ket{\phantom{v}}$, called a \textbf{ket}, indicates a vector.
The symbol inside the ket, whether it be the letter $v$, the name ``Victor'', or any other symbol, is just a name for the vector.
By convention, the vector $\ket{v} + \ket{w}$ can be denoted $\ket{v+w}$.
The name of the vector is often chosen to convey useful information about the vector itself, but keep in mind that the thing in the ket is ultimately just a label.

Two other notations for vectors in common use are $\vec{v}$ and $\mathbf{v}$.
We use the ket notation because it's unambiguous common in physics literature, and lends itself to a useful expression for inner products as we will see below.

\levelstay{Examples of vector spaces}

\begin{enumerate}
  \item[\textit{1}.] The set of all real valued functions defined on the interval $[0, 1]$ over the field of the real numbers.
    This is a vector space because:
    \begin{enumerate}
      \item[1.] Given two functions $f: [0, 1] \to \reals$ and $g: [0, 1] \to \reals$, their sum $h = f + g$ defined by the equation $h(x) = f(x) + g(x)$ is also a function $[0, 1] \to \reals$.
      \item[2.] Multiplying a function $f: [0, 1] \to \reals$ by a scalar $c$ produces a new function $g: [0, 1] \to \reals$ defined by the equation $g(x) = c f(x)$.
      \item[3.] The zero vector is the function $z:[0, 1] \to \reals$ defined by $z(x) = 0$. This is seen because if we define the sum $h = f + z$, then $h(x) = f(x) + z(x) = f(x)$, so $h = f$.
      \item[4.] For any function $f:[0, 1] \to \reals$, its inverse is the function $g:[0, 1] \to \reals$ defined by the equation $g(x) = -f(x)$. This is seen because $(f + g)(x) = f(x) + g(x) = 0$, so $f + g = z$.
      \item[5.] Associativity and distributativity work as expected.
    \end{enumerate}
  \item[\textit{2}.] The set of complex numbers $\ket{z}$, over the field of real numbers.
    Given two complex numbers $\ket{z}$ and $\ket{u}$, then $\ket{z+u} \equiv \ket{z} + \ket{u}$ is also a complex number.
    Given a complex number $\ket{z}$ and a real number $r$, the product $r\ket{z}$ is a complex number.
    The rest of the vector space properties also work, as you should check.
  \item[\textit{3}.] The set of all polynomials of degree seven, over the real numbers.
  \item[\textit{4}.] The set of ordered n-tuples of real numbers ($x_1$, $x_2$,\ldots , $x_n$), over the real numbers.
    In this case we define addition by the equation
    \begin{equation*}
      (x_1, x_2, \ldots, x_n) + (y_1, y_2, \ldots, y_n) = (x_1 + y_1, x_2 + y_2, \ldots, x_n + y_n) \, .
    \end{equation*}
    This space is called $\reals^n$.
\end{enumerate}
The vector space in example 4 is particularly interesting and important because it can be used to represent any finite dimensional real vector space.
For example, the vector space of example 2, the complex numbers over the reals, can be represented by the set of 2-tuples where the first entry represents the real part and the second entry represents the imaginary part.
In this representation a complex number $z$ is written
\begin{equation*}
  z = x + iy \sim (x,y)
\end{equation*}
where the symbol $\sim$ means ``is represented by''.
Obviously the way to add two vectors in this representation is by adding the components, because two complex numbers are added by adding their real and imaginary parts.

The same is true for the set of all polynomials of degree seven, over the real numbers.
Given two such polynomials
\begin{equation*}
  p = p_7 \, x^7 + p_6 \, x^6 + \cdots + p_0 \qquad
  q = q_7 \, x^7 + q_6 \, x^6 + \cdots + q_0
\end{equation*}
their sum is
\begin{equation*}
  p + q = (p_7 + q_7) x^7 + (p_6 + q_6)x^6 + \cdots + (p_0 + q_0)
  \, .
\end{equation*}
We just added the coefficients of each degree of the polynomials, just as if we were adding the components in tuples of real numbers.
Therefore, we can represent the vector space of ``polynomials of degree seven over the real numbers'' by the vector space of ``8-tuples of real numbers, over the real numbers''.

\begin{flushleft} $\clubsuit$ \end{flushleft}
The tuple space $\reals^n$ can be used to represent configurations or ``states'' of physical systems.
The configuration of our box and spring system is given by two numbers: the displacements $x_1$ and $x_2$ of the masses from their equilibrium positions.
Knowledge of these two numbers for all times is the solution of the problem we want to find.
In other words, we say that we've solved the problem if we find the functions
\begin{equation*}
  x_1(t) \qquad \textrm{and} \qquad x_2(t) \, .
\end{equation*}
We see that the state of the system at time $t$ is a vector space if we collect $x_1(t)$ and $x_2(t)$ into a 2-tuple $(x_1(t), x_2(t))$.
Since we showed that tuples of numbers represent vector spaces (remember the examples of the complex numbers and the polynomials of degree seven), we now see that the set of configurations of our spring-and-box system is actually a vector space, and that observation will soon give us a beautiful way to solve for the system's motion.
\begin{flushright} $\clubsuit$ \end{flushright}

\levelstay{Bases and Dimension}

Here we define the notions of vector spaces basis and dimension.

\begin{definition}[Basis]
Consider a finite dimensional vector space \VS.
A basis for \VS, denoted by the symbol $e$, is a set of vectors
\begin{displaymath}
e \equiv \{\ket{e_1}, \ket{e_2}, \ldots, \ket{e_n}\}
\end{displaymath}
with the following three properties:
\begin{enumerate}
\item
For each $i$, $\ket{e_i}$ is a member of \VS.
\item
Any vector $\ket{v}$ in \textbf{V} can be represented as a linear combination,
\begin{displaymath}
\ket{v} = v^e_1 \ket{e_1} + v^e_2 \ket{e_2} + \cdots + v^e_n \ket{e_n} = \sum_{i=1}^n v^e_i \ket{e_i}
\end{displaymath}
where the numbers $v_i^e$ are scalars.
The set of scalars $v_i^e$ are called the \textbf{coefficients} or \textbf{components} of $\ket{v}$ in the $e$ basis.
The linear combination is called the ``decomposition of $\ket{v}$ in basis $e$''.
\item
The linear combination in 2 is unique.
\end{enumerate}
\end{definition}
Note that in the definition above, the letter $e$ refers to the \textit{set} of vectors $\{\ket{e_i}\} \equiv \{ \ket{e_1} \ldots \ket{e_n} \}$ while the symbol $\ket{e_i}$ refers to \emph{one} particular basis vector.
Some books use $\ket{e_i}$ (or perhaps $\mathbf{e}_i$ or $\vec{e}_i$) to refer to the basis itself.
This is abuse of notation because it confuses a single element with a whole set.
In this document, we use either a letter with no subscript, such as $e$, or the set notation $\{\ket{e_i}\}$, to refer to a basis.
Of course, not all bases will be named $e$, we could name a basis $f$, $G$, \textit{Victor}, or anything we want.

\begin{definition}[Dimension]
  Given a vector space \VS\, with basis $e$, the dimension of \VS\, is the number of elements in the set $e$.
\end{definition}
This definition of dimension might seem to depend on the choice of basis, but you can prove that for a given vector space every basis has the same number of elements.

\leveldown{Examples of bases and dimension}

\begin{enumerate}
\item The vectors $\ket{e_1} = \ket{1} = 1$ and $\ket{e_2} = \ket{i} = i$ form a basis for the vector space of complex numbers over the real numbers.
\emph{1}. $1$ and $i$ are both complex numbers, \emph{2}. Any complex number $z$ can be decomposed as
\begin{displaymath}
z = \Re(z) \ket{1} + \Im(z) \ket{i}
\end{displaymath}
where $\Im(z)$ and $\Re(z)$ are both real numbers, \emph{3}. The decomposition is obviously unique.
As there are two basis vectors, the space of complex numbers over the real numbers is two dimensional.\footnote{In this example the vector space is two dimensional because we're using the reals as the field.
If we take the complex numbers over the complex numbers, then a basis could be formed by just $\ket{1}$, because any complex number $z$ could be decomposed as $z = z\,\ket{1}$, and in this case, the space is one dimensional.}
\item The polynomials $\{1, x,x^{2}\}$ form a basis for the space of polynomials of degree two: by definition any polynomial of degree two has the form $a_0 + a_1 x + a_2 x^2$ where the coefficients $a_i$ are uniquely determined by the polynomial in question.
The dimension is 3.
\item For the vector space $\mathbb{R}^n$ we have the obvious basis
\begin{displaymath}
(1,0,\ldots ,0), (0,1,\ldots ,0), \ldots , (0,0,\ldots ,1)
\end{displaymath}
The dimension of $\mathbb{R}^n$ is $n$.
\end{enumerate}

Now let's apply this idea of bases to our physical problem.
\begin{flushleft} $\clubsuit$ \end{flushleft}
We already pointed out that the numbers $x_1(t)$ and $x_2(t)$ can be put into a tuple and considered an element in $\mathbb{R}^2$
So, the configuration space of the box-and-spring system is a vector space, but how is this \emph{useful}?
Well, we can think of equation (\ref{eq:Newton}) as a matrix equation,
\begin{equation} \label{eq:equationOfMotionMatrix}
\left[ \begin{array}{c} \ddot{x}_1(t) \\ \ddot{x}_2(t) \end{array} \right] = \omega^{2}_{0}\left[ \begin{array}{cc} -2 & 1 \\ 1 & -2 \end{array} \right] \left[ \begin{array}{c} x_{1}(t) \\ x_{2}(t) \end{array} \right] \, .
\end{equation}
This matrix equation is exactly equivalent to (\ref{eq:Newton}) as you can check by carrying out the matrix multiplication.
In this matrix equation, The numbers $x_1(t)$ and $x_2(t)$ can be regarded as the components of $\ket{\Psi (t)}$ in a basis $X$ consisting of two vectors $\ket{X_1}$ and $\ket{X_2}$.
$\ket{X_1}$ corresponds to a configuration of the system in which the first mass is displaced one unit of distance to the right of its equilibrium position and mass number two is at it's equilibrium position, and $\ket{X_2}$ is defined analogously.
In this basis, the configuration is written
\begin{equation} \label{eq:Psirep2}
\ket{\Psi (t)} = x_{1}(t)\ket{X_1} + x_{2}(t)\ket{X_2} \, .
\end{equation}

The fact that the differential equations in (\ref{eq:Newton}) are coupled manifests itself in the off-diagonal elements in the matrix of (\ref{eq:equationOfMotionMatrix}).
This suggests that \emph{if we could find a different basis in which the matrix were diagonal, then the differential equations would decouple and the problem would be easily solved}.
Setting up the mathematical tools to understand how to find such a basis is the object of the next section.

There is one more very important remark to be made before we move on.
It might not make any sense at this point, but read it anyway before going on to the next section.
If it doesn't make sense now don't worry, it will very soon.
Equation (\ref{eq:equationOfMotionMatrix}) is a matrix equation written down in a particular basis $X$.
We could rewrite the equation in a more abstract form without reference to any basis.
Let $\ket{\ddot{\Psi}(t)}$ be a vector representing the acceleration of the system at time $t$.
Then we can write
\begin{equation}\label{eq:equationOfMotionAbstract}
\ket{\ddot{\Psi}(t)} = P \ket{\Psi (t)}
\end{equation}
where $P$ is a linear transformation whose representation in basis $X$ is given by the matrix shown in equation (\ref{eq:equationOfMotionMatrix}).
It is crucial that you understand that $P$ is not the matrix in equation (\ref{eq:equationOfMotionMatrix}).
It is some linear transformation on the vector space of configurations of the physical system.
It is \emph{represented} by the matrix in equation (\ref{eq:equationOfMotionMatrix}) in the basis $X$.
In order to decouple the differential equations we want to find a different basis $Y$ in which the matrix representation of $P$ is diagonal.
\begin{flushright} $\clubsuit$ \end{flushright}

\levelstay{Exercises}
\begin{itemize}
\item[1)] Show that if we work in a new basis $Y$ in which the matrix in equation (\ref{eq:equationOfMotionMatrix}) were diagonal then the equations decouple and are easily solved.
\end{itemize}

